SOURCE
https://www.technologyreview.com/s/608777/why-googles-ai-can-write-beautiful-songs-but-still-cant-tell-a-joke/
AGENT
MidiNet
GOAL
User study to compare the melody of eight-bar long generated by MidiNet and by Googleâ€™s MelodyRNN models, each time using the same priming melody
DATA
In order to teach MidiNet before the comparisons were made, it crawled a collection of 1,022 MIDI tabs of pop music from TheoryTab, which provides exactly two channels per tab, one for melody and the other for the underlying chord progression. With this dataset, we can implement at least two versions of MidiNets, one that learns from only the melody channel for fair comparison with MelodyRNN, which does not use chords, and the other that additionally uses chords to condition melody generation,to test the capacity of MidiNet. Both MidiNet  and Google's MelodyRNN created a dataset used specific paramaters, core model, data type, genre specificity as well as mandatory knowledge that the machine had to know before generating any melodies, these were based on priming melody and the music scale and a melody profile.
METHODS
Symbolic Representation for Convolution, Generator CNN and Discriminator CNN, Conditioner CNN
RESULTS
To evaluate the aesthetic quality of the generation result, a user study that involves human listeners was needed. They conducted a study with 21 participants. Ten of whom understand basic music theory and have the experience of being amateur musicians, so they were considered as people with musical backgrounds, the remaining 11 did not have a musical backgrounds at all. Midinet was compared with 3 models realesed by Google Magenta, 100 priming melodies were chosen and each had to create melodies of 8 bars following these primers. The users had to rate the music by 3 metrics, how pleasing, how real, how interesting on a scale of 1 to 5, MelodyRNN used the 3 models lookback RNN, attention RNN and basic RNN , both the lookback RNN and attention RNN outperformed the basic RNN in all 3 metrics and across both groups of users, With MidiNet model 1, this performed alot better than the 3 MelodyRNN models, especially with professionals, with the how pleasing and how real scores remaining the same and the how interesting increasing by 66% on average. The MidiNet model 2 outperformed all 3 of the MelodyRNN but didn't perform as well as the MidiNet model 2.
COMMENTS
It seems that it is very achievable to make melodies that can pass as human made, but as the algorithms get more complex, it seems that professionals can recoginse unnatural patterns and the melodies become more synthetic sounding.
