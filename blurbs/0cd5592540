Example 5:Understanding deep features with computer-generated imagery

SOURCE: http://imagine.enpc.fr/~aubrym/projects/features_analysis/texts/understanding_deep_features_with_CG.pdf
AGENT
 Mathieu Aubry Ecole des Ponts
 Bryan C. Russell Adobe Research  
GOAL
Approach to analyzes the variation of features generated by convolutional neural networks in scene factors that occur in natural images such as 3D viewpoint, color, and scene lighting configuration
 DATA
Images created by CNN's  AlexNet, Places  and Oxford VGG
METHODS
The application begins by rendering a set of stimulus images by varying one or more scene factors. Then they present the stimuli images to a trained CNN as input and record the feature responses for a desired layer. Given the feature responses for the stimuli images they analyzed the principal modes of variation in the feature space via PCA.

RESULTS
They have introduced a method to qualitatively and quantitatively analyze deep features by varying the network stimuli according to factors of interest. Utilizing large collections of 3D models, they have applied this method to compare the relative importance of different factors and have highlighted the difference in sensitivity between the networks and layers to object style, viewpoint and color.

COMMENTS
Another bedrock project which will be used further to improve neural networks ability to render 3d scenes and allow future CNN's in the to produce better 3d rendering.

