SOURCE
Sebastian Raschka, model evaluation, model selection, and algorithm selection in Machine learning
https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html

GOAL
The main goal it to evaluate the performance of the machine learning model. This particular field is important as it helps discover whether or not there is structure in your problem for the algorithms to learn and which algorithms are effective. This reduces the time used in fine tuning the algorithms, in order to make good predictions. In this scenario we run a learning algorithm over a dataset with different settings which will return to us different models and the goal is to pick the best performing models. Due to this reason, the need to estimate the respective performance in order to rank the models against each other is required.

DATA
The data used in this paper can be viewed via this archive.
https://archive.ics.uci.edu/ml/datasets/Iris

METHODS
There are several different approaches to evaluating models. Cross validation is a method of model validation which is generally better than residual. In cross validation we get an indication of how well the algorithm will work when presented with data it has not seen before. One way of resolving this problem is by splitting the training data into two (training around 2/3 and testing around 1/3). This is known as the holdout method. A function approximator fits a particular function using the training set present only. After this, then its asked to predict the output values in the testing set. The main objective is that it has never seen these values before and so it should give us an unbiased estimate of its performance on new unseen data.  The fraction of the correct predictions constitutes our estimate of the prediction accuracy and because we can see how it generalises to the unseen test data we can conclude that the predictions weren't memorized. 

RESULTS
The main results are as follows, the iris dataset which containsof 50 Setosa , 50 versicolor  and 550 virginica flowers, the data is divided into  the training set two thirds (which is 100) and the testing set one third (which is 50). 
The beneficial advantage of this method is that it better than residual method and no longer to compute. The evaluation may depend highly on which data points end up in the training sets and which end up in the test set. For instance looking at our iris data set assuming that the flowers are distributed uniformly in nature, by splitting the data into training and testing sets we introduced an imbalance in the two datasets. In this case we are running and evaluating the model with imbalanced data, which could cause problems with the data analysis.

COMMENTS
I found it interesting that Machine learning can be used to identify patterns in regards to things in nature, such as that of the flower used in this research paper. This shows how vast and versatile machine learning can be, as well as its wide applicability around various scenarios in the modern, changing and fast IT evolving world of today.
