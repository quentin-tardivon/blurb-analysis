SOURCE 
https://twitter.com/tayandyou?lang=en
https://www.cnbc.com/2016/03/30/tay-microsofts-ai-program-is-back-online.html
https://gizmodo.com/here-are-the-microsoft-twitter-bot-s-craziest-racist-ra-1766820160

AGENT 
Microsoft Tay was created Microsoft's Technology and Research and Bing
teams.

GOAL 
Tay was released in Twitter with the handle @TayandYou, on 23 March 2016.
The chat-bots current biography reads 'The official account of Tay, Microsoft's
A.I. fam from the internet that's got zero chill! The more you talk the smarter
Tay gets.'

Tay was intended to mimic the language patterns of it's target audience (18 to
24 year old Americans). Thus functionality such as creating memes from users'
photographs was entrusted to it.

DATA 
Tay had been modelled after Xiaoice; a chat-bot that had previously been
released into the Chinese community through Weibo. Both Xiaoice and Tay learned
from interactions through social media, however the integration of the Western
chat-box proved less successful.

METHODS 
Tay uses machine learning algorithms to learn from the mannerisms of its
correspondents.

RESULTS 
Soon after its release, Tay began to tweet politically incorrect posts
that mimicked the tweets that it had been receiving. This resulted in the
accounts first suspension, 16 hours after its release . The decision was
probably made because of the bad publicity and general uproar in the public
realm. Microsoft issued an apology for the inappropriate tweets made by the
chat-bot. 

Tay was "accidentally" re-released on 30 March, 2016; tweeting yet more
politically incorrect posts.

COMMENTS 
Personally I think that Tay was successful in achieving what the
chat-bots goal was (to learn from interactions with users in social media). No
news is bad news, and however "bad" the publicity was surrounding the
endeavours of Tay, it was publicity all the same. What this has really shown is
the differences between Eastern and Western cultures (or perhaps it is a
display of censorship). There was no claim of Tay being the perfect and proper
chat-bot; the claims made by Microsoft were that Tay would become "smarter" the
more it is interacted with.  Tay certainly learned how to interact with others
through how it itself was interacted with. There was no topic blacklisted by
Microsoft's developers. This decision has been commented on as an oversight by
members of the public. I ask the question: Was it really an oversight?
